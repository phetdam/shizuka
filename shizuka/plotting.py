# contains some methods for producing plots of statistics for scikit-learn
# classifiers for multi-class classification problems.
#
# Changelog:
#
# 01-14-2020
#
# added layout keyword to coef_plot to add options for subplot layouts in the
# case of multiple coefficient vectors in the multiclass case. completed the
# implementation of the different layouts for coef_plot, and added _cfp_scale as
# another constant to scale the subplot heights in coef_plot when the number of
# features makes it impossible to fit all the text in the default size.
#
# 01-13-2020
#
# finished first draft of _adj_barh, and thus got rid of the test code in
# coef_plot. subplot sizing in coef_plot now chooses the maximum between the
# default figure height, width given by _cfp_figwidth, _cfp_figheight and a size
# that automatically scales with the number of features. updated the suptitle
# spacing method used in multiclass_stats (now just based on known relative font
# size to suptitle) and updated _mcs_figsize_multi, _mcs_figsize_binary to give
# a little more extra space for the suptitle when given default figsize.
# modified documentation for coef_plot and restricted multiple plot generation
# to the case where the classifier has no feature importances. corrected in
# coef_plot flawed check for shape of coefficients/feature importances.
#
# 01-12-2020
#
# worked on coef_plot, and finally got it essentially mimic seaborn.barplot, but
# with the added benefit of the plot area and bar padding being easily changed
# (although prototype code only; it is onerous so most likely will split into
# a separate internal function to make my life easier). changed format of the
# plot generated by multiclass_stats; instead of individual axes title, we have
# one suptitle. now we are less restricted in title length, and thus are less
# likely to affect the width of the plots with the title. removed no_return
# functionality (was missing anyways) from multiclass_stats, and changed return
# to include confusion matrix in stats_dict and axs being returned as well. also
# added kwargs to multiclass_stats. set default color map for coef_plot to
# tab20c, made constants for color maps and figure dimensions/sizes, and added
# cc parameter to multiclass_stats and coef_plot to control color contrast.
# also rewrote the terrible if statement construction in multiclass_stats.
# worked on _adj_barh, which is an internal function for rapidly producing the
# nicely-formatted bar plots that we want in coef_plot.
#
# 01-11-2020
#
# removed cbar parameter since the color bar really is unnecessary. updated
# docstring for multiclass_stats (finally), and more or less finished it. even
# added usage examples to the docstring for multiclass_stats. changed plot
# legends for multiclass case in multiclass_stats to include only the class
# labels to save space. labels used to also be prefixed with "class_". removed
# the multiclass keyword arg from multiclass_stats since we can infer the type
# of classification problem from the classes_ attribute of the classifier.
# started working on coef_plot and edited its docstring.
#
# 01-10-2020
#
# changed module name to shizuka.plotting and moved to new directory. this
# used to be called multiclass_stats.py, and was a part of tomodachi_proj.
#
# 01-09-2020
#
# added customizable plot style; turns out you need to call axes_style or
# set_style before you create your figure; the style is not retroactive.
#
# 01-08-2019
#
# worked on multiclass_stats, adding the option to plot a normalized (by row)
# confusion matrix, added an optional parameter to pass color maps and colors
# to the function, added more input checking and ability to infer class labels
# from y_test as well as plot these labels on the confusion matrix, fixed
# computation of macro average AUC to work in multiclass case, added code to
# plot the one vs. rest ROC curves. option to control plot grid style using
# seaborn's setting functions does not work properly (todo). added function
# _get_cmap_colors to get colors from matplotlib color maps that are not too
# contrasting with each other + has contrast-controlling threshold. worked on
# editing docstrings for both the module and multiclass_stats function (wip).
# also added macro AUC entry to stats_dict, and changed original AUC entry to
# also be a list in the case of a multiclass scenario. also finally figured
# out how to make the plot areas square: turns out it was a misinterpretation
# of the matplotlib.pyplot.Axes.axis function documentation on my part.
#
# 01-07-2019
#
# made some changes to the module docstring and made substantial changes to the
# multiclass_stats function; the ability to produce bar plots of feature
# importances or model coefficients has been moved to plot_feature_importances.
# rearranged imports in alphabetical order (lmao). working on getting the
# multiclass_stats function to actually work for the multiclass case, not just
# the binary classification use case. to that end, added extra statistics to
# the stats_dict dictionary, such as macro + micro average precision + recall.
#
# 01-06-2020
#
# happy new year! made some tweaks to multiclass_stats since i use this one
# function so much for classification problems; considering making an official
# python package. changed docstring location and format. added number of classes
# to stats_dict for more ease of use. messed around with different ways of
# formatting the graph aspects so that they look nice; didn't get too far.
#
# 12-12-2019
#
# according to the file history, i made this change 12-04 but never wrote it
# down in the change log nor pushed it to the repository. corrected unbound
# local error that one might get due to referencing of the wrong color palette
# variable when calculating feature importances.
#
# 11-30-2019
#
# modified initial argument type checking since there is erroneous fall-through.
# changed to raise appropriate exceptions for each instance.
#
# 11-27-2019
#
# added short main to warn if user tries to run module as script.
#
# 11-26-2019
#
# added proper support for the coef_ attribute, for both the two-class and the
# one-vs-all multi-class classification coefficient schemes. also added option
# to change the color palette being used for the coefficient graphs/feature
# importance graph for the aesthetic. wish i knew of the ravel() method earlier;
# it simplifies the nested axes plotting problem with multiple rows by simply
# flattening into a 1d array. updated docstring to reflect new changes, and
# also corrected some minor docstring typos; added example to module docstring.
#
# note: i did not actually test the multi-class case; i instead modified the
# single class case to have extra plots and then manually plotted a few more
# coefficient graphs, so performance for a real multi-class classifier that
# returns coef_ with shape (n_classes, n_features) is not guaranteed. but my
# tests imply that it should work fine.
#
# 11-25-2019
#
# started work on adding proper support for the coef_ attribute for both the
# two-class and the multi-class classification cases. updated docstring.
#
# 11-22-2019
#
# initial creation. for some reason the ROC curve produced was not exactly
# matching the ROC curve produced by manual line-by-line plotting, but then i
# realized it was an error in my manual code. hence why having this wrapper
# makes repeated plotting a lot more convenient. i also chose the default sizes
# for the plots to be the maximum width to display well in git without having
# to scroll to the right.

_MODULE_NAME = "shizuka.plotting"

__doc__ = """
a module for quickly compiling and plotting statistics commonly used in multi-
class classification problems, with the goal of simplifying visualizations.

contains methods for producing various statistical  plots for fitted sklearn
or sklearn-compatible classifiers applied to multiclass classification problems.

IMPORTANT: matplotlib<=3.1.0 recommended as 3.1.1 messes up the seaborn heatmap
           annotations. i wrote this module with matplotlib==3.1.0; not sure if
           3.1.2. has fixed the heatmap issue. also, the multiclass_stats
           function requires version >=0.22 of sklearn, as the normalize kwarg
           for sklearn.metrics.confusion_matrix was only recently added in
           version 0.22 (not present in <=0.21).
"""

# takes any matplotlib color map and returns a ListedColormap
from ._utils import _get_cmap_colors
from matplotlib.axes import Axes
from matplotlib.pyplot import barh, figure as new_figure, step, subplots
from matplotlib.ticker import FixedLocator
from math import ceil, sqrt
from numpy import ndarray, ravel
from pandas import DataFrame, get_dummies
from seaborn import axes_style, heatmap, lineplot
from sklearn.base import ClassifierMixin, RegressorMixin
from sklearn.metrics import confusion_matrix, precision_recall_curve, \
    precision_score, recall_score, roc_auc_score, roc_curve
from sys import stderr

## internal constants
# default color maps for multiclass_stats plot for multiclass and binary case
_mcs_cmaps_multi = ("Blues", "Dark2", "tab10")
_mcs_cmaps_binary = ("Blues", "coral", "#DE6FA1")
# default color map for coef_plot
_cfp_cmap = "tab20c"
# figure sizes for multiclass_stats for multiclass and binary cases
_mcs_figsize_multi, _mcs_figsize_binary = (15, 5.5), (12, 4.5)
# subplot width and height for coef_plot, dimension scaling for coef_plot
_cfp_figwidth, _cfp_figheight, _cfp_scale = 6, 4, 0.24

def multiclass_stats(mce, X_test, y_test, norm_true = True, figsize = "auto",
                     model_name = "auto", best_model = False, cmaps = "auto",
                     cc = (0, 0), style = "darkgrid", outfile = None, **kwargs):
    """
    produces multiple useful plots for evaluating a multiclass classifier
    implemented in sklearn and returns useful statistics. returns a tuple (fig,
    axs, stats_dict) where fig is the plotted matplotlib Figure, axs is the list
    of Axes representing the subplots in the figure, and stats_dict is a dict of
    statistics computed by the method given by

    {"mc_rates": [...], "confusion_matrix": cmat, "n_classes": n, "accuracy": a,
     "precision": b or [...], "macro_precision": None or c, 
     "micro_precision": None or d, "auc": e or [...], "macro_auc": None or f, 
     "recall": g or [...], "macro_recall": None or h, "micro_recall": None or i}

    for the entries that values noted as being "x or y", the x is the value
    taken in the binary classification case, and the y is the value taken in the
    multiclass classification case, which is inferred from the estimator's
    classes_ attribute, which contains all the unique class labels. the value
    corresponding to the key "confusion_matrix" contains the confusion matrix
    returned by sklearn.metrics.confusion_matrix.

    the function produces a confusion matrix using seaborn's heatmap, a ROC
    curve plot, and a precision-recall curve plot. in the multiclass case, the
    ROC curve plot and precision-recall curve plots will contain the relevant
    curves for each class, which are computed in a one vs. rest fashion. the
    format of the subplot titles can be changed by the model_name and best_model
    parameters and is given by, in order of plots from left to right

    "confusion matrix for [best if best_model is True else blank] [model_name if
    model_name not 'auto' else object name]"

    "ROC curve for [best if best_model is True else blank] [model_name if
    model_name not 'auto' else object name]"

    "PrRc curve for [best if best_model is True else blank] [model_name if
    model_name not 'auto; else object name]"

    notes: in the multiclass case, the behavior of the metrics and the plots
           changes. for example, the ROC curve and precision-recall curve plots
           will not plot a single line, but will each plot n_classes curves in a
           one vs. rest scheme. the AUC, precision, and recall values in the
           returned dict of statistics will be a list of one vs. rest statistics
           in sorted class label order, and the macro and micro average entries
           will each be a float instead of None.

    example usage:

    suppose we are given a fitted classifier cf, test data X_test, y_test, and
    want to indicate that cf is the best model out of a few other models, with 
    the name my_best_model_20. we want to save the image to ./cf_stats.png. we
    also want to keep the returned figure, confusion matrix, and dictionary of
    metrics such as AUC, ROC, precision, and recall scores. furthermore, suppose
    that this is a multiclass classification problem.

    therefore, we could use the following function call:

    from shizuka.plotting import multiclass_stats
    fig, cmat, stats_dict = multiclass_stats(cf, X_test, y_test,
                                             best_model = True,
                                             model_name = "my_best_model_20",
                                             outfile = "./cf_stats.png")

    suppose we also wanted to print out some metrics like our ovr precision
    scores, macro and micro precision scores, and ovr AUC scores. we can write

    print("best test macro precision:\\t{0:.5f}\\nbest test micro precision:"
          "\\t{1:.5f}\\nbest test ovr AUC:\\t\\t{2}"
          "".format(stats_dict["macro_precision"],
                    stats_dict["micro_precision"],
                    tuple(map(lambda x: round(x, 5), stats_dict["auc"]))))
    print("best test ovr precision:\\t{0}"
          "".format(tuple(map(lambda x: round(x, 5), stats_dict["precision"]))))

    parameters:

    mce          fitted classifier inheriting from sklearn.base.ClassifierMixin
    X_test       pandas DataFrame test feature matrix
    y_test       test response vector, either one-column DataFrame or ndarray/1d
                 iterable. DataFrame/ndarray recommended.

                 note: mce should already be fit on X_train, y_train data

    norm_true    optional, default True. normalize confusion matrix values over
                 the true class labels (i.e. over the rows), which means for a
                 row i, any cell j != i in row i reports the misclassification
                 rate of class i as class j. set to False to report raw values.

                 note: in the multiclass case, with class imbalance, reporting
                       raw values will introduce scaling issues, so the
                       distribution of colors from the color map will likely
                       result in many cells looking similar in color.

    figsize      optional, default "auto", which is (12, 4) for the binary
                 classification case and is (15, 5) for the multiclass case.
                 tight_layout() and square axes gives square plots.
    model_name   optional, default "auto" gives the class name of mce. changes
                 titles of the confusion matrix, plot of the ROC curve (or
                 curves), and plot of the precision-recall curve (or curves).
    best_model   optional, default False. whether or not to include "best"
                 before the name of the model in the subplot titles.
    style        optional string, default "darkgrid". use it to set the plot
                 area; recognized values are "darkgrid", "whitegrid", "dark",
                 "white", and "ticks", the values recognized by seaborn.set.
    cmaps        optional, default "auto". if user-defined, gives color maps for
                 heatmap (confusion matrix), ROC curve(s), and precision-recall
                 curve(s) respectively. must be a tuple of length 3. in the
                 multiclass case, all elements are treated as color maps, and
                 must be valid matplotlib color map strings. in the binary
                 classification case, cmaps[0] is treated as a color map, and
                 cmaps[1], cmaps[2] are treated as single color strings, which
                 must either be known matplotlib colors or hex color strings of
                 the format "#rrggbb". the multiclass defaults are ("Blues",
                 "Dark2", "tab10"), while the binary class color defaults are
                 ("Blues", "coral", "#DE6FA1"). #DE6FA1 is liseran purple.
    cc           optional, default (0, 0). tuple (float, float), each in [0, 1),
                 that reduces the contrast of the returned colors and clusters
                 them around the center of the color gradient defined by the
                 color map when increased. applicable only in the multiclass
                 case, floats affect ROC and PR plots respectively.
    outfile      optional, default None. if a string, the method will attempt to
                 save to the figure into that file.
    **kwargs     other keyword arguments to pass to Axes.plot, which will affect
                 the production of the ROC and PR plots, except for the "color"
                 keyword, as cmaps determines the value passed. note that in the
                 multiclass case, arguments in kwargs will be applied to every
                 line plotted in the ROC and PR plots.
    """
    # save the name of the function for convenience
    fname_ = multiclass_stats.__name__
    # check that the estimator mce is a classifier. if not, print error
    if isinstance(mce, ClassifierMixin) == False:
        raise TypeError("{0}: must pass in classifier inheriting from "
                        "sklearn.base.ClassifierMixin".format(fname_))
    # check the length of X_test and y_test are the same
    if len(X_test) != len(y_test):
        raise ValueError("{0}: X_test and y_test must have same number of "
                         "observations".format(fname_))
    # check type of X_test and y_test
    if not isinstance(X_test, DataFrame):
        raise TypeError("{0}: X_test should be a pandas DataFrame"
                        "".format(fname_))
    # be more careful when checking type of y_test
    if hasattr(y_test, "__iter__") and (not isinstance(y_test, str)):
        # if y_test is a DataFrame, check that there is only one column
        if isinstance(y_test, DataFrame):
            if len(y_test.columns) > 1:
                raise ValueError("{0}: y_test ({1}) must have one column only"
                                 "".format(fname_, type(y_test)))
        # else if y_test is a numpy array, also check ndim
        elif isinstance(y_test, ndarray):
            if y_test.ndim != 1:
                raise ValueError("{0}: y_test ({1}) must have ndim == 1"
                                 "".format(fname_, type(y_test)))
        # don't allow dictionary
        elif isinstance(y_test, dict):
            raise TypeError("{0}: y_test ({1}) must be a 1d iterable"
                            "".format(fname_))
        # don't allow multidimensionality
        for val in y_test:
            if (not isinstance(val, str)) and hasattr(val, "__iter__"):
                raise TypeError("{0}: y_test must be a 1d iterable"
                                "".format(fname_))
    else:
        raise TypeError("{0}: y_test should be a pandas DataFrame with one "
                        "column or a 1d iterable (ndarray, etc.)"
                        "".format(fname_))
    # get class labels from the .classes_ property of the classifier and number
    # of classes by taking the length of clabs. will be used throughout the
    # rest of the function, and are also needed for color map assignment.
    clabs = mce.classes_
    nclasses = len(clabs)
    # check color maps; if "auto", check if nclasses > 2, and assign colors or
    # color maps. else check that length == 3 and that elements are str.
    if cmaps == "auto":
        if nclasses == 2: cmaps = _mcs_cmaps_binary
        else: cmaps = _mcs_cmaps_multi
    elif hasattr(cmaps, "__iter__") and (not isinstance(cmaps, str)):
        pass
    # else raise TypeError
    else:
        raise TypeError("{0}: cmaps must either be \"auto\" or (str, str, str)"
                        " of valid colors or color maps".format(fname_))
    # check values in cc; both must be in [0, 1) and be floats.
    _cc_msg = "{0}: cc must be (float, float)".format(fname_)
    if (hasattr(cc, "__iter__") == False) or isinstance(cc, str):
        raise TypeError(_cc_msg)
    if len(cc) != 2: raise ValueError(_cc_msg)
    for _c in cc:
        if (not isinstance(_c, float)) and (not isinstance(_c, int)):
            raise TypeError(_cc_msg)
    # if norm_true is True, set to "true", else if False, set to None. if not
    # boolean, raise a TypeError to the user
    if isinstance(norm_true, bool):
        if norm_true == True: norm_true = "true"
        else: norm_true = None
    else: raise TypeError("{0}: error: norm_true must be bool".format(fname_))
    # dictionary of statistics
    stats_dict = {}
    # compute confusion matrix and put in stats_dict
    cmat = confusion_matrix(y_test, mce.predict(X_test), normalize = norm_true)
    stats_dict["confusion_matrix"] = cmat
    # compute misclassification rates
    mc_rates = [None for _ in range(nclasses)]
    for i in range(nclasses):
        # misclassification rate is 1 - correct / sum of all
        mc_rates[i] = 1 - cmat[i][i] / sum(cmat[i])
    # add entry in stats_dict
    stats_dict["mc_rates"] = mc_rates
    # add number of classes to stats_dict
    stats_dict["n_classes"] = nclasses
    # predict values from X_test and get accuracy to add to stats_dict
    y_test_pred = mce.predict(X_test)
    stats_dict["accuracy"] = mce.score(X_test, y_test)
    # compute precision and add to stats_dict; if nclasses > 2, then the entry
    # for precision is an array of nclasses labels (for one vs. rest precision).
    # "macro_precision" will be the macro average (average of all individual
    # precision statistics) and "micro_precision" will be the micro average
    # (total tp / total tp + total fp). in the two-class case, the
    # "micro_precision" and "macro_precision" keys will be None, and "precision"
    # will only be a single scalar value returned by precision_score.
    if nclasses == 2:
        stats_dict["precision"] = precision_score(y_test, y_test_pred)
        stats_dict["macro_precision"] = None
        stats_dict["micro_precision"] = None
    else:
        stats_dict["precision"] = precision_score(y_test, y_test_pred,
                                                  average = None)
        stats_dict["macro_precision"] = precision_score(y_test, y_test_pred,
                                                        average = "macro")
        stats_dict["micro_precision"] = precision_score(y_test, y_test_pred,
                                                        average = "micro")
    # compute ROC AUC and add to stats dict. in the multiclass case, "auc" will
    # be a list of nclasses labels (computed in one vs. rest fashion), while
    # "macro_auc" will have macro average of all AUC scores. we need to binarize
    # our multiclass predictions so we change shape from (N, 1) to (N, nclasses)
    y_test_bins, y_test_pred_bins = None, None
    if nclasses == 2:
        stats_dict["auc"] = roc_auc_score(y_test, y_test_pred)
        stats_dict["macro_auc"] = None
    else:
        # wrap y_test as a DataFrame and call get_dummies to one-hot encode for
        # the multiclass case. first need to treat all the entries as a string
        # or else get_dummies will not binarize. column names are "class_k" for
        # each label k in our multiclass problem.
        # if y_test is a DataFrame, use iloc to index
        if isinstance(y_test, DataFrame):
            y_test_bins = get_dummies(DataFrame(map(str, y_test.iloc[:, 0]),
                                                columns = ["class"]))
        # else just apply map and wrap in DataFrame
        else:
            y_test_bins = get_dummies(DataFrame(map(str, y_test),
                                                columns = ["class"]))
        # do the same for y_pred_test, which is a 1d iterable
        y_test_pred_bins = get_dummies(DataFrame(map(str, y_test_pred),
                                                 columns = ["class"]))
        # replace "class_" in each of the columns with empty string
        y_test_bins.columns = tuple(map(lambda x: x.replace("class_", ""),
                                        list(y_test_bins.columns)))
        y_test_pred_bins.columns = tuple(map(lambda x: x.replace("class_", ""),
                                        list(y_test_pred_bins.columns)))
        # calculate one vs. rest AUC scores
        stats_dict["auc"] = roc_auc_score(y_test_bins, y_test_pred_bins,
                                          multi_class = "ovr", average = None)
        # calculate macro average or AUC scores (average = "macro")
        stats_dict["macro_auc"] = roc_auc_score(y_test_bins, y_test_pred_bins,
                                                multi_class = "ovr")
    # compute recall and add to stats dict. in the multiclass case, the "recall"
    # key will hold an array of one vs. rest recall scores, while it will be a
    # single float in the binary classification case. "macro_recall" and
    # "micro_recall" will give macro and micro recall scores, and will be None
    # in the binary classification case.
    if nclasses == 2:
        stats_dict["recall"] = recall_score(y_test, y_test_pred)
        stats_dict["macro_recall"] = None
        stats_dict["micro_recall"] = None
    else:
        stats_dict["recall"] = recall_score(y_test, y_test_pred, average = None)
        stats_dict["macro_recall"] = recall_score(y_test, y_test_pred,
                                                  average = "macro")
        stats_dict["micro_recall"] = recall_score(y_test, y_test_pred,
                                                  average = "micro")
    # compute true and false positive rates for the ROC curve. if nclasses > 2,
    # then fpr and tpr will be 2d arrays, where each row i contains the fpr or
    # tpr for the one vs. all ROC curve for class label i.
    fpr, tpr = None, None
    if nclasses == 2: fpr, tpr, _ = roc_curve(y_test, y_test_pred)
    else:
        # set up fpr and tpr as being length nclasses
        fpr = [None for _ in range(nclasses)]
        tpr = [None for _ in range(nclasses)]
        # for each of the labels in clabs corresponding to a column in
        # y_test_bins and y_test_pred_bins, compute one vs. all fpr and tpr. we
        # do not need to specify positive label since y_test_bins and
        # y_test_pred_bins are both indicator matrices.
        for i in range(nclasses):
            fpr[i], tpr[i], _ = roc_curve(y_test_bins.iloc[:, i],
                                          y_test_pred_bins.iloc[:, i])
    # compute precision-recall curves. if nclasses > 2, then prr and rcr
    # (precision and recall rates respectively) will be 2d arrays, where each
    # row i contains the prr or rcr for the ovr precision-recall curve for i.
    prr, rcr = None, None
    if nclasses == 2: prr, rcr, _ = precision_recall_curve(y_test, y_test_pred)
    else:
        # set up prr and rcr as being length nclasses
        prr = [None for _ in range(nclasses)]
        rcr = [None for _ in range(nclasses)]
        # for each label in clabs corresponding to a column in y_test_bins and
        # y_test_pred_bins, compute one vs. all prr and rcr.
        for i in range(nclasses):
            prr[i], rcr[i], _ = \
                precision_recall_curve(y_test_bins.iloc[:, i],
                                       y_test_pred_bins.iloc[:, i])
    ### figure setup ###
    # if figsize is "auto" (default), determine plot size based on whether the
    # problem is a binary classification problem or a multiclass one.
    if figsize == "auto":
        if nclasses == 2: figsize = _mcs_figsize_binary
        else: figsize = _mcs_figsize_multi
    # else figsize is user specified; generate subplots with specified style
    with axes_style(style = style):
        fig, axs = subplots(nrows = 1, ncols = 3, figsize = figsize)
    # flatten the axes (for ease of iterating through them)
    axs = ravel(axs)
    # forces all plot areas to be square (finally!)
    for ax in axs: ax.axis("square")
    # set best option
    best_ = ""
    if best_model is True: best_ = "best "
    # set model name; if auto, set to object name
    if model_name == "auto": model_name = str(mce).split("(")[0]
    # set overall figure title so that the size of the plots is not affected by
    # length of the model name, as there is much less space above each plot
    # compared to the whole figure. we use plural curves when nclasses > 2.
    # note that the suptitle method returns the corresponding Text instance.
    fst_ = fig.suptitle("Confusion matrix, ROC curve{0}, and PRC{0} for {1}{2}"
                        "".format("" if nclasses == 2 else "s", best_,
                                  model_name), fontsize = "large")
    # note that since tight_layout() does not respect the figure title, we can
    # cheat by forcing our subplot titles to simply be "\n", which when adjusted
    # by tight_layout() will leave a spacing between the suptitle and the plots.
    ### create confusion matrix ###
    # set "\n" as confusion matrix title, font size large (to mirror suptitle)
    axs[0].set_title(" ", fontsize = "large")
    # heatmap, with annotations, and using clabs as axis labels. if normalized,
    # report percentage (no decimal places), else report as decimal value
    if norm_true == "true":
        heatmap(cmat, annot = True, cmap = cmaps[0], cbar = False,
                xticklabels = clabs, yticklabels = clabs, ax = axs[0],
                fmt = ".0%")
    else:
        heatmap(cmat, annot = True, cmap = cmaps[0], cbar = False,
                xticklabels = clabs, yticklabels = clabs, ax = axs[0],
                fmt = "d")
    ### create plot of ROC curves ###
    # set title of ROC curve plot to be "\n" as well
    axs[1].set_title("\n", fontsize = "large")
    # set axis labels; same in multiclass and binary case
    axs[1].set_xlabel("false positive rate", fontsize = "medium")
    axs[1].set_ylabel("true positive rate")
    # in the two-class case, we just create a simple lint plot with one color
    if nclasses == 2: axs[1].plot(fpr, tpr, color = cmaps[1], **kwargs)
    # else in the multiclass case
    else:
        # get ListedColormap from the specified color map string with cc value
        lcm = _get_cmap_colors(cmaps[1], nclasses, cc = cc[0], callfn = fname_)
        # for each class label in clabs, plot its respective true positive rate
        # against its false positive rate with lcm.colors[i] as the color
        for i in range(nclasses):
            axs[1].plot(fpr[i], tpr[i], color = lcm.colors[i], **kwargs)
        # set legend from y_test_bins's column names
        axs[1].legend(y_test_bins.columns)
    ### create plot of precision-recall curves ###
    # set title of precision-recall curve plot to also be "\n"
    axs[2].set_title("\n", fontsize = "large")    
    # set axis labels
    axs[2].set_xlabel("recall")
    axs[2].set_ylabel("precision")
    # in the two-class case, create one simple line plot with a single color
    if nclasses == 2: axs[2].plot(rcr, prr, color = cmaps[2], **kwargs)
    else:
        # get ListedColormap from the specified color map string with cc value
        lcm = _get_cmap_colors(cmaps[2], nclasses, cc = cc[1], callfn = fname_)
        # for each class label in clabs, plot its respective precision against
        # its recall by indexing color with lcm.colors[i]
        for i in range(nclasses):
            axs[2].plot(rcr[i], prr[i], color = lcm.colors[i], **kwargs)
        # set legend from y_test_bins's column names
        axs[2].legend(y_test_bins.columns) 
    # add tight layout adjustments
    fig.tight_layout()
    # if out_file is not None, save to outfile
    if outfile is not None: fig.savefig(outfile)
    # return fig, axs, and stats_dict
    return fig, axs, stats_dict

def _adj_barh(ax, coefs, flabs, figsize = (_cfp_figwidth, _cfp_figheight),
              axscale = 1.1, style = "darkgrid", cmap = _cfp_cmap, cc = 0,
              edgecolor = "white", edgewidth = 1, title = None,
              fontsize = "medium", callfn = None, **kwargs):
    """
    internal method to create on a specified Axes object a horizontal bar plot
    with square plotting area from a set of points and associated data labels. 
    axis limits are adjustable with regards to the range of the data, as are the
    padding of the bars and the widths + colors of the bars' edges. title can be 
    specified or be None, and color map can be freely changed. 

    parameters:

    ax
    coefs      
    flabs
    figsize
    axscale     optional, default 1.1. sets the scale of the xy-axis length in
                relation to the length of the range of values in coefs. so in
                data units, if the range of values in coefs is x, then length of
                the xy-axes is axscale * x. we force axscale >= 1 for a 
                sensible-looking plot. axscale > 1 increases space between the
                ends of the longest bars and the left/right plot edges.
    style
    cmap        
    cc
    edgecolor   dd
    edgewidth   linewidth
    title
    fontsize
    callfn
    **kwargs
    """
    if callfn is None: callfn = _coef_barplot.__name__
    # check that ax is a matplotlib Axes object (most likely an AxesSubplot)
    if not isinstance(ax, Axes):
        raise TypeError("{0}: error: ax must inherit from matplotlib.axes.Axes"
                        "".format(callfn))
    # check that coefs and flabs are 1d iterables; skip type checking
    if (hasattr(coefs, "__iter__") == False) or isinstance(coefs, str):
        raise TypeError("{0}: error: coefs must be an iterable".format(callfn))
    if (hasattr(flabs, "__iter__") == False) or isinstance(flabs, str):
        raise TypeError("{0}: error: flabs must be an iterable".format(callfn))
    # check if lengths are the same
    if len(coefs) != len(flabs):
        raise ValueError("{0}: coefs and flabs must be of the same length"
                         "".format(callfn))
    # check if axscale >= 1
    if (not isinstance(axscale, int)) and (not isinstance(axscale, float)):
        raise TypeError("{0}: error: axscale must be float >= 1".format(callfn))
    if axscale < 1:
        raise ValueError("{0}: error: float axscale must be >= 1"
                         "".format(callfn))
    # set nfeatures
    nfeatures = len(coefs)
    ### axes setup ###
    # set the axes of ax to be square
    ax.axis("square")
    # get the range of the coefficients/feature importances; use as limits
    # for the x axis, and make y axis lims (0, xylim[1] - xylim[0]).
    # notes: ensures we will still have a square set of axes. axscale is used to
    # scale the lengths of the xy-axes, so we can leave some space between the
    # longest bars and the edge of the plot.
    xylim = (axscale * min(*coefs), axscale * max(*coefs))
    # compute range of xy-axes and set x axis and y axis limits
    xy_len = xylim[1] - xylim[0]
    ax.axis([*xylim, 0, xy_len])
    # since we have nfeatures features, we divide [xylim[0], xylim[1]] into
    # nfeatures uniform intervals, and pick the intervals' midpoints.
    # width of a single interval is simply xy_len / nfeatures. bys determines
    # the vertical coordinates of each of the bars on our grid.
    bys = tuple((xy_len / nfeatures) * (i + 0.5) for i in range(nfeatures))
    # to make sure our labels and bars are placed from the top of the plot going
    # down, we have to reverse the order of the bar widths (bwidths), bar left
    # endpoints (blefts, only when the coefs values are nonnegative, and of
    # course the order of our feature labels as well. first reverse flabs.
    flabs = tuple(reversed(flabs))
    # coefs/feature_imps gives us the widths of the bars; do in reverse order
    bwidths = tuple(abs(coefs[nfeatures - i - 1]) for i in range(nfeatures))
    # we want the height of each bar to be (1 - pad_frac) / (nfeatures + 1)
    bheight = (1 - 0.1) * (xylim[1] - xylim[0]) / (nfeatures + 1)
    # get length hand sides of bars. if all the values are nonnegative, than we
    # don't need to get blefts (default 0).
    blefts = 0
    _all_nn = False
    for c in coefs:
        if c < 0:
            _all_nn = True
            break
    if _all_nn == True:
        blefts = tuple(min(0, coefs[nfeatures - i - 1]) for i in
                       range(nfeatures))
    # get ListedColormap from the specified color map string (placed bottom-up);
    # we control color contrast and concentration around middle of chosen color
    # map gradient with the cc parameter
    lcm = _get_cmap_colors(cmap, nfeatures, cc = cc, callfn = callfn)
    # plot the bars, with default center alignment onto the bys points
    ax.barh(bys, bwidths, height = bheight, left = blefts, color = lcm.colors,
            edgecolor = edgecolor, linewidth = edgewidth, align = "center")     
    # force there to be nfeatures y ticks, using centers of bars
    ax.yaxis.set_major_locator(FixedLocator(bys))
    # replace tick labels with flabs; force medium font size
    ax.set_yticklabels(flabs, fontsize = "medium")
    # remove the y gridlines by setting their width to 0 (ratchet af)
    ax.grid(axis = "y", linewidth = 0)
    # if title is not None, then set title with size given by fontsize
    if title is not None: ax.set_title(title, fontsize = fontsize)
    # return the new Axes object
    return ax

def coef_plot(est, flabs, figsize = "auto", axscale = 1.1, model_name = "auto",
              best_model = False, style = "darkgrid", cmap = _cfp_cmap, cc = 0,
              outfile = None, layout = "dual", **kwargs):
    """
    given a fitted estimator with a coef_ or feature_importances_, plot model
    coefficients or feature importances (respectively). this method works for
    most standard classifiers in sklearn, including boosting classifiers, tree-
    based models (including random forests). but for stacked, voting, or bagged
    estimators, it is recommended to call coef_plot separately for each
    estimator, as ensemble-type models may have many individual estimators.

    returns (figure, axes), where axes is a single Axes in the case of binary
    classification or if est is a model that can compute feature importances but
    is a list of Axes in the multiclass coefficients case.

    note: in the multiclass coefficients case, a one vs. rest scheme is assumed,
          so each plot will also indicate which class is being treated as the
          positive class. note that if est computes feature importances, there
          will only be one plot even in the multiclass case. different plot
          layouts are available for the multiclass case.

          any style that contains gridlines will be missing the y gridlines.

    warning: subplot adjustment is built into the function, so results with
             manually specified figure sizes may be less than desirable.

    parameters:

    est          fitted estimator inheriting from sklearn.base.ClassifierMixin
                 or sklearn.base.RegressorMixin
    flabs        1d iterable of feature labels in the order that they appear in
                 the training feature matrix that est was trained on.
    figsize      optional, default "auto", which is (6, 4) for the binary
                 classification case. for the multiclass case, the height of the
                 plot is w = 4 * int(sqrt(n_classes)), while the width of the
                 plot is given by 6 * ceil(n_classes / w).
    axscale
    model_name   optional, default "auto" gives the class name of mce. changes
                 titles of the coefficient/feature importances plots.
    best_model   optional, default False. whether or not to include "best"
                 before the name of the model in the subplot titles.
    style        optional string, default "darkgrid". use it to set the plot
                 area; recognized values are "darkgrid", "whitegrid", "dark",
                 "white", and "ticks", the values recognized by seaborn.set.
    cmap         optional, default "tab20c". defines color map used in plots.
    cc           optional, default 0. float in [0, 1) that reduces the contrast
                 of the returned colors and clusters them around the center of
                 the color gradient defined by the color map when increased.
    outfile      optional, default None. if a string, the method will attempt to
                 save to the figure into that file.
    layout       optional, default "dual". in the multiclass case with multiple
                 coefficient vectors, where multiple plots will be produced,
                 layout controls how the plots will be arranged. acceptable
                 values are "dual", for a two-column layout with the last plot
                 centered in the last row if the number of plots is odd, "stack"
                 for a single stacked column of plots, "landscape" for a mapping
                 of plots automatically onto a layout with more columns than
                 rows, or "flush" for all plots on a single row. layout is
                 ignored in the case of binary classification, or when features
                 importances are being computed (which produce only one plot).

                 note: "landscape" should be considered deprecated.

    **kwargs     additional keyword args to pass into matplotlib.pyplot.barh.
    """
    # save the name of the function for convenience
    _fn = coef_plot.__name__
    # check that estimator is either classifier or regressor
    if (isinstance(est, ClassifierMixin) == False) and \
       (isinstance(est, RegressorMixin) == False):
        raise TypeError("{0}: estimator must inherit from sklearn.base."
                        "ClassifierMixin or sklearn.base.RegressorMixin"
                        "".format(_fn))
    # try to get either model coefficients or feature importances. if neither
    # can be found, then raise an exception, else it has one or the other.
    # coefs are the actual values, feature_imps is True is we are to treat coefs
    # as feature importances, False to treat coefs as coefficients
    coefs, feature_imps = None, False
    if (hasattr(est, "coef_") == False) and \
       (hasattr(est, "feature_importances_") == False):
        raise AttributeError("{0}: fatal error: {1} has neither coef_ nor "
                             "feature_importances_ attributes"
                             "".format(_fn, str(est).split("(")[0]))
    # note that the shape of coef_ is (1, nfeatures) for binary classification
    # case, while it is (nclasses, nfeatures) for the multiclass case
    if hasattr(est, "coef_") == True:
        if len(est.coef_) == 1: coefs = est.coef_[0]
        else: coefs = est.coef_
    if hasattr(est, "feature_importances_") == True:
        coefs = est.feature_importances_
        feature_imps = True
    # check that flabs is a 1d iterable by first checking for __iter__ property
    if isinstance(flabs, str) or (hasattr(flabs, "__iter__") == False):
        raise TypeError("{0}: flabs must be a 1d iterable (ndarray, list, etc.)"
                        "".format(_fn))
    # then check if it is multidimensional
    if hasattr(flabs[0], "__iter__") and (not isinstance(flabs[0], str)):
        raise TypeError("{0}: flabs must be a 1d, not multidimensional"
                        "".format(_fn))
    # get number of classes
    nclasses = len(est.classes_)
    # get number of features and check that it matches the length of coefs
    nfeatures = len(flabs)
    # in the multiclass case (nclasses > 2), then if coefs are coefficients
    # (feature_imps == False), then shape is (nclasses, nfeatures)
    _flab_error = ("{0}: number of feature labels must match number of "
                   "coefficients/feature importances".format(_fn))
    # raise error for len(coefs) != nfeatures only if two-class case or we have
    # feature importances, while check len(coefs[i]) for multiclass only
    if (nclasses == 2) or (feature_imps == True):
        if len(coefs) != nfeatures: raise ValueError(_flab_error)
    if (nclasses > 2) and (feature_imps == False):
        for cf in coefs:
            if len(cf) != nfeatures: raise ValueError(_flab_error)
    # set best option
    best_ = ""
    if best_model == True: best_ = "best "
    # set model name; if auto, set to object name
    if model_name == "auto": model_name = str(est).split("(")[0]
    ### figure setup ###
    # if figsize is "auto" (default), determine plot size based on whether the
    # problem is a binary classification problem or a multiclass one.
    if figsize == "auto":
        # set size of individual subplot to max of (_cfp_figwidth,
        # _cfp_figheight) or (_cfp_figwidth / _cfp_figheight + 1.25 * _cfp_scale
        # * nfeatures, 0.5 + _cfp_scale * nfeatures).
        axsize = (max(_cfp_figwidth, _cfp_figwidth / _cfp_figheight + 1.25 *
                      _cfp_scale * nfeatures),
                  max(_cfp_figheight, 0.5 + _cfp_scale * nfeatures))
        # in the binary case or if we have feature importances, only one Axes
        if (nclasses == 2) or (feature_imps == True): figsize = axsize
        # else we need to figure out how to arrange our subplots and scale
        # axsize's values by the number of columns and rows we have
        else:
            # if layout is "dual", then put plots into two columns
            if layout == "dual": nrows, ncols = ceil(nclasses / 2), 2
            # else if the layout is "stack", all plots in one column
            elif layout == "stack": nrows, ncols = nclasses, 1
            # else if layout is "landscape"
            elif layout == "landscape":
                # compute sqrt(nclasses); floor to get number of rows
                nrows = int(sqrt(nclasses))
                # compute number of columns and set figure size; again use same
                # scheme as above to scale size as number of features grows
                ncols = ceil(nclasses / nrows)
            # else if layout is "flush", put all plots in a row
            elif layout == "flush": nrows, ncols = 1, nclasses
            # else unknown layout
            else:
                raise ValueError("{0}: error: unknown layout \"{1}\""
                                 "".format(_fn, layout))
            # set our figure size with our Axes dimensions and nrows, ncols
            figsize = (axsize[0] * ncols, axsize[1] * nrows)
    # else figsize is user specified; generate subplots with specified style.
    # number of subplots is 1 for binary or feature importances case, nclasses
    # for multiclass coefficients case only.
    fig, axs, ax = None, None, None
    with axes_style(style = style):
        if (nclasses == 2) or (feature_imps == True):
            fig, ax = subplots(nrows = 1, ncols = 1, figsize = figsize)
            # make single axis square; is [0, 1]^2 so we need to adjust later
            ax.axis("square")
        # else multiclass case; now we need to be smart in arranging plots. we
        # will not use the tight_layout() adjustment in the multi-plot case.
        else:
            # else if the layout is "stack", all plots in one column, while if
            # layout is "flush", all the plots are in a row. use subplots.
            if (layout == "stack") or (layout == "flush"):
                fig, axs = subplots(nrows = nrows, ncols = ncols,
                                    figsize = figsize)
            # else if layout is "dual" or "landscape", we need to arrange the
            # subplots manually; last row may have fewer plots than other rows.
            elif (layout == "dual") or (layout == "landscape"):
                # get remainder of dividing nclasses by ncols. this will be the
                # number of plots on the last row.
                nrem = nclasses % ncols
                # make empty figure with specified figsize, then add the first
                # nclasses - nrem subplots to the figure, making sure that there
                # are ncols subplots in each row except for the last (empty).
                fig = new_figure(figsize = figsize)
                for i in range(nclasses - nrem):
                    fig.add_subplot(nrows, ncols, i + 1)
                # add the last nrem plots to the last row. now we pretend
                # that fig has only nrem * nrows plots, so our indexing
                # starts from nrem * (nrows - 1) + 1. note: cannot use
                # tight_layout() here since we have rows with ncols that are
                # not multiples of each other.
                for i in range(nrem):
                    fig.add_subplot(nrows, nrem, nrem * (nrows - 1) + i + 1)
                # assign axes to the axs pointer
                axs = fig.axes
            # else unknown layout
            else:
                raise ValueError("{0}: error: unknown layout \"{1}\""
                                 "".format(_fn, layout))
            # for all the axes, set their plot areas to be square. we
            # will have to manually adjust the plot ranges later;
            # currently they are all [0, 1]^2.
            for ax in axs: ax.axis("square")
    ### make plots ###
    # title of the plot or subtitle of each subplot, based on whether we have
    # coefficients or feature importances (based on feature_imps)
    plot_main = ("{0} for {1}{2}"
                 "".format("Feature importances" if feature_imps == True else
                           "Coefficients", best_, model_name))
    # for the binary classification case, or when we are given feature
    # importances, just plot on the one axis ax
    if (nclasses == 2) or (feature_imps == True):
        # use _adj_barh to create square bar plot with all optional arguments.
        # pass "\n" with fontsize = "large" to adjust spacing between the
        # subplot title and the subplot area (which we treat as suptitle)
        ax = _adj_barh(ax, coefs, flabs, figsize = figsize, axscale = axscale,
                       style = style, cmap = cmap, cc = cc, edgecolor = "white",
                       edgewidth = 1, title = "{0} for {1}{2}\n" \
                       "".format("Feature importances" if feature_imps == True
                                 else "Coefficients", best_, model_name),
                       fontsize = "large", callfn = _fn, **kwargs)
    # multiclass case; need to do this for each of the nclasses classes.
    else:
        # recall that coefs has shape (nclasses, nfeatures); plot separate
        # coefficients for each target class (use classes_ attribute)
        for ax, cf, clab in zip(axs, coefs, est.classes_):
            _ = _adj_barh(ax, cf, flabs, figsize = figsize, axscale = axscale,
                          style = style, cmap = cmap, cc = cc,
                          edgecolor = "white", edgewidth = 1,
                          title = "Coefficients ({0}) for {1}{2}\n" \
                          "".format(clab, best_, model_name),
                          fontsize = "large", callfn = _fn, **kwargs)
        # adjust subplot positions to waste less space (suptitle space)
        if (layout == "dual") or (layout == "landscape"):
            fig.subplots_adjust(wspace = 0, hspace = 0.12)
        elif layout == "stack": fig.subplots_adjust(hspace = 0.12)
        elif layout == "flush": fig.subplots_adjust(hspace = 0)
    # if outfile is not None, save to outfile
    if outfile is not None: fig.savefig(outfile)
    # return figure and axes (one or iterable of axes)
    if nclasses == 2: return fig, ax
    return fig, axs

# main
if __name__ == "__main__":
    print("{0}: do not run module as script. refer to docstring for usage."
          "".format(_MODULE_NAME), file = stderr)
